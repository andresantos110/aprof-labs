{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "torch.random.manual_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a sequence-to-sequence network that reverses strings with the help of attention. We will randomly generate strings consisting of \"a\", \"b\", \"c\", and \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "raw_vocab = list(\"abcd\")\n",
    "itos = [BOS, EOS] + raw_vocab\n",
    "stoi = {n: i for i, n in enumerate(itos)}\n",
    "vocab_size = len(itos)  # Plus BOS/EOS\n",
    "\n",
    "N = 200\n",
    "valid_size = 100\n",
    "\n",
    "def sample_string(min_length, max_length):\n",
    "    length = random.randrange(min_length, max_length)\n",
    "    return \"\".join([random.choice(raw_vocab) for _ in range(length)])\n",
    "\n",
    "def sample_strings(min_length, max_length, size):\n",
    "    return [sample_string(min_length, max_length) for _ in range(size)]\n",
    "\n",
    "def to_tensor(name):\n",
    "    indices = [stoi[BOS]] + [stoi[n] for n in name] + [stoi[EOS]]\n",
    "    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def make_dataset(lines):\n",
    "    dataset = [(to_tensor(line), to_tensor(reversed(line))) for line in lines]\n",
    "    return dataset\n",
    "\n",
    "train_lines = sample_strings(3, 15, N)\n",
    "valid_lines = sample_strings(3, 15, valid_size)\n",
    "\n",
    "train_dataset = make_dataset(train_lines)\n",
    "valid_dataset = make_dataset(valid_lines)\n",
    "\n",
    "print(train_lines[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the model is an RNN-based encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, bidirectional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden_size //= 2\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_size, \n",
    "            hidden_size, \n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"\n",
    "        input (LongTensor): batch x src length\n",
    "        src length (batch-length list0: If given, the input will be packed\n",
    "        hidden: hidden or hidden/cell state input dimensions for the RNN type\n",
    "        returns:\n",
    "            output (FloatTensor): batch x src length x hidden size\n",
    "            hidden_n (FloatTensor): hidden or hidden/cell state input\n",
    "                dimensions for the RNN type\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(input)\n",
    "        output, hidden_n = self.rnn(emb, hidden)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden_n = self._reshape_hidden(hidden_n)\n",
    "        return output, hidden_n\n",
    "\n",
    "    def _merge_tensor(self, state_tensor):\n",
    "        forward_states = state_tensor[::2]\n",
    "        backward_states = state_tensor[1::2]\n",
    "        return torch.cat([forward_states, backward_states], 2)\n",
    "\n",
    "    def _reshape_hidden(self, hidden):\n",
    "        \"\"\"\n",
    "        hidden:\n",
    "            num_layers * num_directions x batch x self.hidden_size // 2\n",
    "            or a tuple of these\n",
    "        returns:\n",
    "            num_layers\n",
    "        \"\"\"\n",
    "        assert self.rnn.bidirectional\n",
    "        if isinstance(hidden, tuple):\n",
    "            return tuple(self._merge_tensor(h) for h in hidden)\n",
    "        else:\n",
    "            return self._merge_tensor(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a decoder. This implementation works both with and without an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, attn=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, input, context, hidden):\n",
    "        \"\"\"\n",
    "        input (LongTensor): batch x tgt length\n",
    "        context (FloatTensor): batch x src length x hidden size\n",
    "        hidden: hidden or hidden/cell state input dimensions for the RNN type\n",
    "        returns (FloatTensor): (batch*tgt length) x output size\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(input)\n",
    "        output, hidden_n = self.rnn(emb, hidden)\n",
    "\n",
    "        alignment = None\n",
    "        # apply attention between source context and query from\n",
    "        # decoder RNN\n",
    "        if self.attn is not None:\n",
    "            output, alignment = self.attn(output, context)\n",
    "\n",
    "        flat_output = output.contiguous().view(-1, self.rnn.hidden_size)\n",
    "        return self.output_layer(flat_output), alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put them together into an encoder-decoder model class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src, tgt (LongTensor): (batch size x sequence length)\n",
    "        returns (FloatTensor): (batch*tgt length) x output size\n",
    "        \"\"\"\n",
    "        context, enc_hidden = self.encoder(src)\n",
    "        return self.decoder(tgt, context=context, hidden=enc_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our base model defined, we can write training and validation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, loss, optimizer):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    random.shuffle(train_iter)  # present examples in random order\n",
    "    for src, tgt in train_iter:\n",
    "        model.zero_grad()\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        pred, _ = model(src, tgt_in)\n",
    "        gold = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        batch_loss = loss(pred, gold)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, data_iter):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_iter:\n",
    "            tgt_in = tgt[:, :-1]\n",
    "            pred = model(src, tgt_in)[0].argmax(dim=1)\n",
    "            gold = tgt[:, 1:].contiguous().view(-1)\n",
    "            n_correct += (pred == gold).sum().item()\n",
    "            n_total += gold.size(0)\n",
    "        return n_correct / n_total\n",
    "\n",
    "\n",
    "def train(model, train, valid, epochs=30, learning_rate=0.5):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    epochs = list(range(1, epochs + 1))\n",
    "    for epoch in epochs:\n",
    "        print('Training epoch {}'.format(epoch))\n",
    "        train_loss = train_epoch(model, train, loss, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_acc = validate(model, valid)\n",
    "        valid_accs.append(valid_acc)\n",
    "        print('Train loss: {} ; Validation acc: {}'.format(train_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a unidirectional model without attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = vocab_size\n",
    "hidden_size = 64\n",
    "\n",
    "enc = Encoder(vocab_size, embedding_size, hidden_size)\n",
    "dec = Decoder(vocab_size, embedding_size, hidden_size)\n",
    "enc.embeddings.weight.data = torch.eye(vocab_size)\n",
    "dec.embeddings.weight.data = enc.embeddings.weight.data\n",
    "enc.embeddings.weight.requires_grad = False\n",
    "dec.embeddings.weight.requires_grad = False\n",
    "\n",
    "model = Seq2Seq(enc, dec)\n",
    "print(model)\n",
    "\n",
    "train(model, train_dataset, valid_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model often manages to predict the right sequence, but it also often fails. Note that the decoder makes use of the *last* hidden state from the encoder, which has recently seen the final time step of the source sequence (in other words, the first element it needs to predict), but the other elements less recently. If only there were a way to make it easier for the model to focus on less recent positions...\n",
    "\n",
    "The attention mechanism is an extra layer for the decoder that can do precisely this. In this exercise, we consider a simple but effective attention mechanism called *dot product attention*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProdAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        \"\"\"\n",
    "        query: batch x tgt_length x hidden_size\n",
    "        context: batch x src_length x hidden_size\n",
    "        \"\"\"\n",
    "\n",
    "        return attn_h_t, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that attention has been implemented, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = DotProdAttention(hidden_size)\n",
    "enc = Encoder(vocab_size, embedding_size, hidden_size, bidirectional=True)\n",
    "dec = Decoder(vocab_size, embedding_size, hidden_size, attn=attn)\n",
    "enc.embeddings.weight.data = torch.eye(vocab_size)\n",
    "dec.embeddings.weight.data = enc.embeddings.weight.data\n",
    "enc.embeddings.weight.requires_grad = False\n",
    "dec.embeddings.weight.requires_grad = False\n",
    "\n",
    "attn_model = Seq2Seq(enc, dec)\n",
    "print(attn_model)\n",
    "\n",
    "train(attn_model, train_dataset, valid_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the model's attention matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "string = \"abacadabacc\"  # try something\n",
    "reversed_string = reversed(string)\n",
    "\n",
    "src = to_tensor(string)\n",
    "tgt = to_tensor(reversed_string)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, alignment = attn_model(src, tgt)\n",
    "\n",
    "attn_matrix = alignment.squeeze(0).numpy()\n",
    "plt.matshow(attn_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
